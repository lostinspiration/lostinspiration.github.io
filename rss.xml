<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Lost Inspiration</title><description>A Memoir</description><link>https://www.kichka.dev/</link><language>en</language><item><title>Deforestation (Logging)</title><link>https://www.kichka.dev/posts/deforestation/</link><guid isPermaLink="true">https://www.kichka.dev/posts/deforestation/</guid><description>Logging â€” The Stack, Fundamentals, and the hard won experience that has continued to pay dividens. With bonus content of extending it into SQL Server</description><pubDate>Tue, 30 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Nothing is more frustrating than a system being broken with no visibility into it, at the same time there is nothing more satisfying than catching problems with your vendors before they do. Being proactive, rather than reactive also buys a ton of grace from your users when you catch problems before they are reported and sometime having gone totally unnoticed.&lt;/p&gt;
&lt;p&gt;This reminds me of a quote from the &amp;lt;a href=&quot;https://www.imdb.com/title/tt0756880/&quot; target=&quot;_blank&quot;&amp;gt;Futurama Godfellas&amp;lt;/a&amp;gt; episode where the Galaxy God Entity is talking to Bender and says &lt;em&gt;&quot;When you do things right, people won&apos;t be sure you&apos;ve done anything at all&quot;&lt;/em&gt; and I couldn&apos;t agree more. Proper logging and visibility will lead to exactly that!&lt;/p&gt;
&lt;p&gt;Below we will look at how I have implemented it and the choices made along the way.&lt;/p&gt;
&lt;h1&gt;The Software Stack&lt;/h1&gt;
&lt;h2&gt;&amp;lt;a href=&quot;https://grafana.com/oss/grafana/?pg=get&amp;amp;plcmt=selfmanaged-box1-cta2&quot; target=&quot;_blank&quot;&amp;gt;Grafana&amp;lt;/a&amp;gt;&lt;/h2&gt;
&lt;p&gt;This is the user interface to query logs and metrics, creating rich dashboards and visualizations, and alerting when specific conditions are met.&lt;/p&gt;
&lt;h2&gt;&amp;lt;a href=&quot;https://grafana.com/oss/loki/?pg=get&amp;amp;plcmt=selfmanaged-box2-cta2&quot; target=&quot;_blank&quot;&amp;gt;Loki&amp;lt;/a&amp;gt;&lt;/h2&gt;
&lt;p&gt;This is a platform specifically optimized for handling message streams. I prefer to instrument my applications using JSON messages that Grafana can then parse out with &amp;lt;a href=&quot;https://grafana.com/docs/loki/latest/query/log_queries/&quot; target=&quot;_blank&quot;&amp;gt;LogQL&amp;lt;/a&amp;gt;. Become familiar with writing log queries as that is how you will explore the data.&lt;/p&gt;
&lt;p&gt;Log streams are grouped by labels. These labels have served me quite well for many years.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Application&lt;/strong&gt; is the top level assembly or application that produced the log&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt; is the environment &lt;em&gt;(e.g. dev, stage, production)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MachineName&lt;/strong&gt; is the computer or server running the application
:::warning
Loki will group and store in like chunks (files) on the server using the stream labels. You want this to be as &lt;strong&gt;low cardinality&lt;/strong&gt; as possible to avoid having too many individual files on the server.
:::&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might be asking at this point, &quot;How do I add additional information to my log messages&quot;? This is where the Log Property Stack comes in. It is a way to enrich your logs with additional context without trying to jam it into the log message itself.&lt;/p&gt;
&lt;h4&gt;Log Property Stack&lt;/h4&gt;
&lt;p&gt;Instead of adding all the details to the log message itself, you should utilize the property stack which will persist the property with every message nested within the scope that property was defined.&lt;/p&gt;
&lt;p&gt;Here are some examples of what I use through my applications. These are staples that I include in everything and add more as needed to determine application state at major branching points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;InstanceId&lt;/strong&gt; for long running process, defering to CorrelationId for nested units of work&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CorrelationId&lt;/strong&gt; for discrete units of work&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RequestInfo&lt;/strong&gt; object for web stuff like url, headers, query params, and other information to help trace requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Dependency Recommendations&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Serilog&lt;/strong&gt; for dotnet applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Serilog.Sinks.Debug&lt;/strong&gt; for web applications when you are running locally&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Serilog.Sinks.Console&lt;/strong&gt; for console applications writing to stdout/stderr&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Serilog.Sinks.Grafana.Loki&lt;/strong&gt; for writing to a Loki server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SerilogTimings&lt;/strong&gt; for timing operations, use this whenever my application leaves its domain of control &lt;em&gt;(e.g. when making database query or web requests)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom stored procedures&lt;/strong&gt; for SQL Server, &lt;em&gt;see &lt;a href=&quot;#bonus&quot;&gt;bonus content&lt;/a&gt; below&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&amp;lt;a href=&quot;https://grafana.com/oss/prometheus/?pg=get&amp;amp;plcmt=selfmanaged-box11-cta2&quot; target=&quot;_blank&quot;&amp;gt;Prometheus&amp;lt;/a&amp;gt;&lt;/h2&gt;
&lt;p&gt;Prometheus is a time series database and handles things that do not fit into structured logging such as performance counters. Like Loki, has its own query language called &amp;lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/querying/basics/&quot; target=&quot;_blank&quot;&amp;gt;PromQL&amp;lt;/a&amp;gt;.&lt;/p&gt;
&lt;h4&gt;Performance counters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;CPU and Memory Utilization&lt;/li&gt;
&lt;li&gt;Disk and Network Usage&lt;/li&gt;
&lt;li&gt;IIS and SQL specific counters like request throughput&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These counters can be gathered from the servers directly using a tool called &amp;lt;a href=&quot;https://github.com/prometheus-community/windows_exporter&quot; target=&quot;_blank&quot;&amp;gt;windows_exporter&amp;lt;/a&amp;gt; that provides an HTTP endpoint that can be scraped by a Prometheus server.
:::warning
Please be mindful of your companies security practices as the out of the box the server is not secured. Configuration can be found &amp;lt;a href=&quot;https://github.com/prometheus/exporter-toolkit/blob/master/docs/web-configuration.md&quot; target=&quot;_blank&quot;&amp;gt;here&amp;lt;/a&amp;gt;.
:::&lt;/p&gt;
&lt;h1&gt;Logging Fundamentals&lt;/h1&gt;
&lt;p&gt;:::caution[Here be dragons...]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid logging PII -- Logs are public and can leak information&lt;/li&gt;
&lt;li&gt;Avoid excessive logging -- Logging can be great at first to build confidence in a process. If you find yourself logging every little step, &lt;code&gt;Debug&lt;/code&gt; or &lt;code&gt;Verbose&lt;/code&gt; should be used&lt;/li&gt;
&lt;li&gt;Avoid logging user correctable errors as &lt;code&gt;Error&lt;/code&gt;. &lt;code&gt;Debug&lt;/code&gt; or &lt;code&gt;Warning&lt;/code&gt; may be a more appropriate place for these types of messages&lt;/li&gt;
&lt;li&gt;Avoid punctuation when possible. Log messages are &lt;strong&gt;fragments, not sentences&lt;/strong&gt;
:::&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Log Key Events&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Major branching points in your code&lt;/li&gt;
&lt;li&gt;When errors or unexpected values are encountered&lt;/li&gt;
&lt;li&gt;Any IO or resource intensive operations&lt;/li&gt;
&lt;li&gt;Significant domain events&lt;/li&gt;
&lt;li&gt;Request failures and retires&lt;/li&gt;
&lt;li&gt;Beginning and end of time-consuming batch operations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Choose Appropriate Logging Level&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Be generous with your logging but strict with your logging levels. In almost all cases the level of your logs should be Debug&lt;/li&gt;
&lt;li&gt;Use Information for log events that would be needed in production to determine the running state or correctness of your application&lt;/li&gt;
&lt;li&gt;Use Warning or Error for unexpected events like exceptions&lt;/li&gt;
&lt;li&gt;The Error level should be reserved for events that you intend to act on. User correctable errors should never be logged at this level&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Bonus -- Logging in SQL Server&lt;/h1&gt;
&lt;h2&gt;Configure&lt;/h2&gt;
&lt;p&gt;This is called at the start of your SQL session that you want to log and should only be called once per session.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[ConfigureFor](
	@Application VARCHAR(200),
	@Environment VARCHAR(11)
)
AS
BEGIN
	BEGIN TRY
		-- normalize and validate environment value
		SELECT @Environment = CASE
			WHEN @Environment IN (&apos;development&apos;, &apos;0&apos;) THEN &apos;Development&apos;
			WHEN @Environment IN (&apos;stage&apos;, &apos;2&apos;) THEN &apos;Stage&apos;
			WHEN @Environment IN (&apos;production&apos;, &apos;1&apos;) THEN &apos;Production&apos;
			ELSE NULL
		END

		IF @Environment IS NULL
		BEGIN
			RAISERROR(&apos;Invalid environment value. Should be one of [Development|Stage|Production]&apos;, 16, 1)
		END

		DECLARE @ReadOnly INT = 1
		EXECUTE sp_set_session_context &apos;Application&apos;, @Application, @ReadOnly
		EXECUTE sp_set_session_context &apos;MachineName&apos;, @@SERVERNAME, @ReadOnly
		EXECUTE sp_set_session_context &apos;Environment&apos;, @Environment, @ReadOnly

		EXECUTE sp_set_session_context &apos;__index__&apos;, NULL
	END TRY
	BEGIN CATCH
		THROW
	END CATCH
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Log Level Functions&lt;/h2&gt;
&lt;p&gt;The meat and potatoes of the implementation. It will gather all of the log properties off the stack and do the submission to the Loki server. There are convenience functions for each log level.&lt;/p&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;p&gt;:::important
This will need a way to make HTTP requests from SQL Server. This could be through &amp;lt;a href=&quot;https://learn.microsoft.com/en-us/sql/database-engine/configure-windows/ole-automation-procedures-server-configuration-option&quot; target=&quot;_blank&quot;&amp;gt;OLEAutomation&amp;lt;/a&amp;gt;, a dotnet SQLCLR assembly or SQL Server Language Extensions.
:::
:::note
This is typically not called directly favoring the convenience functions.
:::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Impl](
	@Message NVARCHAR(4000),
	@Level NVARCHAR(50)
)
AS
BEGIN
	BEGIN TRY
		--DECLARE @Message NVARCHAR(4000) = &apos;DONKEY!&apos;
		-- cheap insurance to make sure that ConfigureFor has been called
		IF SESSION_CONTEXT(N&apos;Application&apos;) IS NULL
		BEGIN
			RETURN
		END

		-- get log time as unix epoch in nanos
		DECLARE @Time BIGINT = DATEDIFF_BIG(NANOSECOND, &apos;1970-01-01 00:00:00.0000000&apos;, SYSUTCDATETIME())

		-- add standard set of properties to be logged
		EXECUTE LogProperty_Add &apos;Message&apos;, @Message
		EXECUTE LogProperty_Add &apos;level&apos;, @Level

		-- send to loki
		-- json
		-- {
		-- 	&quot;streams&quot;: [
		-- 	{
		-- 		&quot;stream&quot;: {
		-- 			&quot;label&quot;: &quot;value&quot;
		-- 		},
		-- 		&quot;values&quot;: [
		-- 			[ &quot;unix epoch in nanoseconds&quot;, &quot;log line&quot; ],
		-- 			[ &quot;unix epoch in nanoseconds&quot;, &quot;log line&quot; ]
		-- 		]
		-- 	}
		-- 	]
		-- }
		-- 
		-- http://localhost:3100/loki/api/v1/push

		-- these are configured in the ConfigureFor stored procedure
		DECLARE @StreamLabels NVARCHAR(MAX) = (
			SELECT *
			FROM (
				SELECT *
				FROM(VALUES
					(&apos;Application&apos;, SESSION_CONTEXT(N&apos;Application&apos;)), 
					(&apos;MachineName&apos;, SESSION_CONTEXT(N&apos;MachineName&apos;)), 
					(&apos;Environment&apos;, SESSION_CONTEXT(N&apos;Environment&apos;))
				) stream_labels([key], [value])
			) a
			PIVOT (
				MAX([value]) FOR [key] IN ([Application], [MachineName], [Environment])
			) pvt
			FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER/*, INCLUDE_NULL_VALUES*/
		)
		--SELECT @StreamLabels

		IF OBJECT_ID(&apos;tempdb..#TMP&apos;) IS NOT NULL
		BEGIN
			DROP TABLE #TMP
		END

		SELECT
			[key] = [value],
			[value] = SESSION_CONTEXT(CAST([value] AS NVARCHAR(100))),
			[is_json] = ISJSON(CAST(SESSION_CONTEXT(CAST([value] AS NVARCHAR(100))) AS NVARCHAR(MAX)))
		INTO #TMP
		FROM OPENJSON(CAST(SESSION_CONTEXT(N&apos;__index__&apos;) AS NVARCHAR(MAX)))

		-- generate select list to account for json column data.
		-- we have to use JSON_QUERY on json data so that it is treated as a json fragment when the overall select is serialized
		DECLARE @SelectList NVARCHAR(MAX) = (SELECT STUFF((
		-- stuff select begin
		SELECT &apos;,&apos; + IIF([is_json] = 1, QUOTENAME([key]) + &apos; = JSON_QUERY(CAST(&apos; + QUOTENAME([key]) + &apos; AS NVARCHAR(MAX)))&apos;, QUOTENAME([key]))
		FROM #TMP
		FOR XML PATH (&apos;&apos;)
		-- stuff select end
		), 1, 1, &apos;&apos;)
		)
		--SELECT @SelectList

		-- generate pivot list
		DECLARE @PvtList NVARCHAR(MAX) = (SELECT STUFF((
		-- stuff select begin
		SELECT &apos;,&apos; + QUOTENAME([key]) FROM #TMP
		FOR XML PATH (&apos;&apos;)
		-- stuff select end
		), 1, 1, &apos;&apos;)
		)
		--SELECT @PvtList

		-- generate message span
		DECLARE @Sql NVARCHAR(MAX) = &apos;&apos;
		SELECT @Sql = &apos;
		DECLARE @Json NVARCHAR(MAX) = (
			SELECT {1}
			FROM (
				SELECT [key], [value]
				FROM #TMP
			) a
			PIVOT (
				MAX([value]) FOR [key] IN ({0})
			) pvt
			FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER/*, INCLUDE_NULL_VALUES*/
		)
		SELECT @Json&apos;
		SELECT @Sql = REPLACE(@Sql, &apos;{0}&apos;, @PvtList)
		SELECT @Sql = REPLACE(@Sql, &apos;{1}&apos;, @SelectList)

		DECLARE @Json TABLE ([Json] NVARCHAR(MAX))
		INSERT INTO @Json
		EXECUTE(@Sql)

		-- generate full loki request
		DECLARE @LokiRequest NVARCHAR(MAX) = (
			SELECT
				[stream] = JSON_QUERY(@StreamLabels),
				[values] = JSON_QUERY(&apos;[[&apos; + QUOTENAME(CAST(@Time AS VARCHAR(20)), &apos;&quot;&apos;) + &apos;,&quot;&apos; + STRING_ESCAPE([Json], &apos;json&apos;) + &apos;&quot;]]&apos;)
			FROM @Json
			FOR JSON AUTO, ROOT(&apos;streams&apos;)
		)
		--SELECT @LokiRequest
		--PRINT @LokiRequest

		-- reset exception
		EXECUTE LogProperty_Add &apos;Exception&apos;, NULL

		-- loose the request
		DECLARE @HeadersJson NVARCHAR(MAX) = (
			SELECT *
			FROM (
				SELECT 
					[content-type] = &apos;application/json&apos;,
					[content-length] = CAST(LEN(@LokiRequest) AS VARCHAR(20))
			) a
			FOR JSON AUTO
		)
		--SELECT @HeadersJson

		EXECUTE HttpPost &apos;http://localhost:3100/loki/api/v1/push&apos;, @LokiRequest, NULL, NULL, @HeadersJson
	END TRY
	BEGIN CATCH
		-- swallow
	END CATCH
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Fatal&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Fatal](
	@Message NVARCHAR(4000),
	@CallingProc SYSNAME = NULL
)
AS
BEGIN
	IF @CallingProc IS NOT NULL
	BEGIN
		EXECUTE LogProperty_AddSourceContext @CallingProc
	END

	-- extra error details
	IF ERROR_MESSAGE() IS NOT NULL
	BEGIN
		DECLARE @Exception NVARCHAR(4000)
		SELECT @Exception = (
			SELECT *
			FROM (
				SELECT 
					[ErrorNumber] = ERROR_NUMBER(),
					[ErrorSeverity] = ERROR_SEVERITY(),
					[ErrorState] = ERROR_STATE(),
					[ErrorProcedure] = ERROR_PROCEDURE(),
					[ErrorLine] = ERROR_LINE(),
					[ErrorMessage] = ERROR_MESSAGE()
			) exception
			FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER
		)

		EXECUTE LogProperty_Add &apos;Exception&apos;, @Exception
	END

	EXECUTE LogTo_Impl @Message, &apos;critical&apos;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Error&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Error](
	@Message NVARCHAR(4000),
	@CallingProc SYSNAME = NULL
)
AS
BEGIN
	IF @CallingProc IS NOT NULL
	BEGIN
		EXECUTE LogProperty_AddSourceContext @CallingProc
	END

	-- extra error details
	IF ERROR_MESSAGE() IS NOT NULL
	BEGIN
		DECLARE @Exception NVARCHAR(4000)
		SELECT @Exception = (
			SELECT *
			FROM (
				SELECT 
					[ErrorNumber] = ERROR_NUMBER(),
					[ErrorSeverity] = ERROR_SEVERITY(),
					[ErrorState] = ERROR_STATE(),
					[ErrorProcedure] = ERROR_PROCEDURE(),
					[ErrorLine] = ERROR_LINE(),
					[ErrorMessage] = ERROR_MESSAGE()
			) exception
			FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER
		)

		EXECUTE LogProperty_Add &apos;Exception&apos;, @Exception
	END

	EXECUTE LogTo_Impl @Message, &apos;error&apos;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Debug&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Debug](
	@Message NVARCHAR(4000),
	@CallingProc SYSNAME = NULL
)
AS
BEGIN
	IF @CallingProc IS NOT NULL
	BEGIN
		EXECUTE LogProperty_AddSourceContext @CallingProc
	END

	EXECUTE LogTo_Impl @Message, &apos;debug&apos;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Warning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Warning](
	@Message NVARCHAR(4000),
	@CallingProc SYSNAME = NULL
)
AS
BEGIN
	IF @CallingProc IS NOT NULL
	BEGIN
		EXECUTE LogProperty_AddSourceContext @CallingProc
	END

	EXECUTE LogTo_Impl @Message, &apos;warn&apos;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Information&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Information](
	@Message NVARCHAR(4000),
	@CallingProc SYSNAME = NULL
)
AS
BEGIN
	IF @CallingProc IS NOT NULL
	BEGIN
		EXECUTE LogProperty_AddSourceContext @CallingProc
	END

	EXECUTE LogTo_Impl @Message, &apos;info&apos;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Verbose&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CREATE PROCEDURE [dbo].[LogTo_Verbose](
	@Message NVARCHAR(4000),
	@CallingProc SYSNAME = NULL
)
AS
BEGIN
	IF @CallingProc IS NOT NULL
	BEGIN
		EXECUTE LogProperty_AddSourceContext @CallingProc
	END

	EXECUTE LogTo_Impl @Message, &apos;trace&apos;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Log Property Stack&lt;/h2&gt;
&lt;p&gt;This is how you add additional context to your logs with some convenience functions below. They are just key value pairs that will get added to the log stream and can be parsed out using the Loki logQL within Grafana.&lt;/p&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ALTER PROCEDURE [dbo].[LogProperty_Add](
	@Name NVARCHAR(100),
	@Value NVARCHAR(4000)
)
AS
BEGIN
	BEGIN TRY
		--EXECUTE sp_set_session_context &apos;__index__&apos;, &apos;[&quot;ApplicationInstance&quot;, &quot;CorrelationId&quot;]&apos;
		--DECLARE @Name VARCHAR(100) = &apos;InstanceId&apos;

		-- you cannot query values from the SESSION_CONTEXT without explicitly knowing the key
		-- for this reason, we house a list of keys in a __index__ session value so we know what
		-- to pull when we generate the actual log
		-- add new key to property bag
		DECLARE @IndexKeys NVARCHAR(4000)
		SELECT @IndexKeys = &apos;[&apos; + ISNULL(STUFF(
-- stuff select begin
(SELECT &apos;,&quot;&apos; + [Value] + &apos;&quot;&apos;
FROM (
	SELECT [value]
	FROM OPENJSON(CAST(SESSION_CONTEXT(N&apos;__index__&apos;) AS NVARCHAR(MAX)))
	UNION
	SELECT @Name
) a([value])
FOR XML PATH (&apos;&apos;)),
-- stuff select end
		1, 1, &apos;&apos;), &apos;&apos;) + &apos;]&apos;
		
		-- persist new property bag
		EXECUTE sp_set_session_context &apos;__index__&apos;, @IndexKeys
		--SELECT SESSION_CONTEXT(N&apos;__index__&apos;)

		-- persist log property
		EXECUTE sp_set_session_context @Name, @Value
	END TRY
	BEGIN CATCH
		-- swallow
	END CATCH
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;CorrelationId&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ALTER PROCEDURE [dbo].[LogProperty_CorrelationId](
	@Value NVARCHAR(4000)
)
AS
BEGIN
	EXECUTE LogProperty_Add &apos;CorrelationId&apos;, @Value
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;SourceContext&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ALTER PROCEDURE [dbo].[LogProperty_AddSourceContext](
	@Value NVARCHAR(4000)
)
AS
BEGIN
	EXECUTE LogProperty_Add &apos;SourceContext&apos;, @Value
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;InstanceId&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ALTER PROCEDURE [dbo].[LogProperty_InstanceId](
	@Value NVARCHAR(4000)
)
AS
BEGIN
	EXECUTE LogProperty_Add &apos;InstanceId&apos;, @Value
END
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item><item><title>SQL Server Quirky Update</title><link>https://www.kichka.dev/posts/sql-quirky-update/</link><guid isPermaLink="true">https://www.kichka.dev/posts/sql-quirky-update/</guid><description>Controversial running total calculations in SQL Server using the &quot;Quirky Update&quot;</description><pubDate>Sun, 28 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;While I was developing my trading algo, I wanted to calculate different technical indicators, most of which depend on previous data points. In order to do that in SQL Server you have a couple options, and this is one of them.&lt;/p&gt;
&lt;p&gt;There are two key points that make this &quot;quirky update&quot; work.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You &lt;strong&gt;MUST&lt;/strong&gt; have a clustered index on the table that represents the &lt;strong&gt;expected sort order&lt;/strong&gt; of the data. In this case is was the tick data ascending.&lt;/li&gt;
&lt;li&gt;You &lt;strong&gt;MUST&lt;/strong&gt; use &lt;code&gt;OPTION (MAXDOP 1)&lt;/code&gt; option on the query so SQL Server doesn&apos;t try to parallelize the update.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;p&gt;Below we create a staging table to house the technical indicators, the primary key is the clustered index, and tick data is inserted into the table in sorted order. You could also have the clustered index on the &lt;code&gt;[Time]&lt;/code&gt; column as well.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--DROP TABLE [stage].[Meta]
CREATE TABLE [stage].[Meta] (
	[RowIdx] BIGINT IDENTITY PRIMARY KEY,
	[Id] UNIQUEIDENTIFIER,
	[InstrumentId] INT,
	[Pip] NUMERIC(10, 5),
	[Time] DATETIME2,
	[Open] NUMERIC(10, 5),
	[High] NUMERIC(10, 5),
	[Low] NUMERIC(10, 5),
	[Close] NUMERIC(10, 5),
	[Volume] INT
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the following examples take note of the usage of the variables that retain the previous calculation result and how they get used when updating the calculation for the next row. This essentially causes the same behavior you would get if you had used a SQL &lt;code&gt;CURSOR&lt;/code&gt;.&lt;/p&gt;
&lt;h1&gt;Example -- Heikin Ashi Candlestick&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;/* indicator columns
ALTER TABLE [stage].[Meta]
ADD	[HaOpen] NUMERIC(10, 5),
    [HaHigh] NUMERIC(10, 5),
    [HaLow] NUMERIC(10, 5),
    [HaClose] NUMERIC(10, 5)
--*/

-- calculate heikin ashi candles
-- NOTE: variable ordering matters here
---------------------------------------------------------------------------------------------------
DECLARE
    @HaOpen NUMERIC(10, 5) = 0,
    @HaClose NUMERIC(10, 5) = 0,
    @PrevHaOpen NUMERIC(10, 5) = 0,
    @PrevHaClose NUMERIC(10, 5) = 0

-- quirky update --
UPDATE [stage].[Meta]
SET @HaClose = ([Open] + [High] + [Low] + [Close]) / 4.0,
    [HaClose] = @HaClose,

    @HaOpen = CASE WHEN [RowIdx] = 1 THEN ([Open] + [Close]) / 2.0 ELSE (@PrevHaOpen + @PrevHaClose) / 2.0 END,
    [HaOpen] = @HaOpen,

    [HaHigh] = (SELECT MAX([val]) FROM ( VALUES ([High]), ([HaOpen]), ([HaClose]) ) T([val])),
    [HaLow] = (SELECT MIN([val]) FROM ( VALUES ([Low]), ([HaOpen]), ([HaClose]) ) T([val])),

    @PrevHaClose = @HaClose,
    @PrevHaOpen = @HaOpen
OPTION (MAXDOP 1);
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Example -- Exponential Moving Average&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;/* indicator columns
ALTER TABLE [stage].[Meta]
ADD [EMA012] NUMERIC(10, 5),
    [EMA026] NUMERIC(10, 5)
--*/

-- calculate ema&apos;s
-- NOTE: depends on sma being calculated first
---------------------------------------------------------------------------------------------------
-- quirky update --
DECLARE
    @EMA012 NUMERIC(10, 5) = 0,
    @EMA026 NUMERIC(10, 5) = 0

UPDATE [stage].[Meta]
SET @EMA012 = CASE WHEN [RowIdx] = 1 THEN [SMA012] ELSE ([Close] - @EMA012) * (2 / ((12 + 1) * 1.0)) + @EMA012 END,
    [EMA012] = @EMA012,

    @EMA026 = CASE WHEN [RowIdx] = 1 THEN [SMA026] ELSE ([Close] - @EMA026) * (2 / ((26 + 1) * 1.0)) + @EMA026 END,
    [EMA026] = @EMA026
OPTION (MAXDOP 1);
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;I first came across this when looking for a better way to perform the update without having all of the ceremony that comes with the &lt;code&gt;CURSOR&lt;/code&gt;. You can find the original article below and their 10 rules on using this technique.&lt;/p&gt;
&lt;p&gt;&amp;lt;a href=&quot;https://www.sqlservercentral.com/articles/solving-the-running-total-and-ordinal-rank-problems-rewritten&quot; target=&quot;_blank&quot;&amp;gt;The Rules &amp;amp; Inspiration&amp;lt;/a&amp;gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CLUSTERED INDEX MUST BE PRESENT IN THE CORRECT ORDER&lt;/li&gt;
&lt;li&gt;PARALLELISM MUST BE PREVENTED: MUST include OPTION (MAXDOP 1)&lt;/li&gt;
&lt;li&gt;DON&apos;T WORK AGAINST PARTITIONED STRUCTURES&lt;/li&gt;
&lt;li&gt;USE THE TABLOCKX HINT&lt;/li&gt;
&lt;li&gt;DO NOT USE JOINS&lt;/li&gt;
&lt;li&gt;YOU MUST HAVE AN &quot;ANCHOR&quot; COLUMN&lt;/li&gt;
&lt;li&gt;DO NOT USE ORDER BY&lt;/li&gt;
&lt;li&gt;DO NOT USE INDEX HINTS TO TRY TO FORCE ORDER&lt;/li&gt;
&lt;li&gt;GET IT RIGHT&lt;/li&gt;
&lt;li&gt;TEST!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item><item><title>SQL Voodoo Black Magic</title><link>https://www.kichka.dev/posts/sql-voodoo-blackmagic/</link><guid isPermaLink="true">https://www.kichka.dev/posts/sql-voodoo-blackmagic/</guid><description>SQL snips for querying sysobjects table and using the results to dynamically generate SQL query</description><pubDate>Sat, 27 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Sometimes you want to be able to dynamically construct table definitions to perform different actions, such as creating a copy of a table on a different database, compare table schema, sync table data where the tables are not guarenteed to have the same schema, generate update, merge, select and insert statements and not have to keep adjusting the code as the table schema drifts.&lt;/p&gt;
&lt;p&gt;Below we will go through a few of these scenarios, but first lets define the &quot;black magic&quot; that the rest of the examples will build upon.&lt;/p&gt;
&lt;h1&gt;Black Magic&lt;/h1&gt;
&lt;p&gt;These are the exact same query but produce a &lt;code&gt;#Source&lt;/code&gt; and &lt;code&gt;#Destination&lt;/code&gt; temp tables that the examples will use.&lt;/p&gt;
&lt;h2&gt;Select Source Schema&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;DECLARE
	@TableName SYSNAME = &apos;MyTable&apos;,
	@TableSchema SYSNAME = &apos;dbo&apos;

IF OBJECT_ID(&apos;tempdb..#Source&apos;) IS NOT NULL
	DROP TABLE #Source

SELECT
	[ColumnId] = sc.[column_id],
	[ColumnName] = sc.[name] COLLATE DATABASE_DEFAULT,
	-- adjust timestamp type to varbinary
	[Type] = CASE WHEN st.[user_type_id] = 189 THEN &apos;varbinary&apos; ELSE st.[name] END COLLATE DATABASE_DEFAULT,
	-- adjust nvarchar length to half what is reported since it takes 2 bytes to store unicode, and adjust timestamp to the translated varbinary length
	[Length] = CASE WHEN sc.[max_length] = -1 THEN -1 WHEN st.[user_type_id] = 231 THEN sc.[max_length] / 2 WHEN st.[user_type_id] = 189 THEN 85 ELSE sc.[max_length] END,
	[Precision] = sc.[precision], 
	[Scale] = sc.[scale],
	[IsNullable] = sc.[is_nullable]
INTO #Source
FROM [sys].[objects] so
JOIN [sys].[columns] sc
ON so.[object_id] = sc.[object_id]
JOIN [sys].[types] st
ON sc.[user_type_id] = st.[user_type_id]
JOIN [sys].[schemas] ss
ON so.[schema_id] = ss.[schema_id]
WHERE so.[name] = @TableName AND ss.[name] = @TableSchema
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Select Destination Schema&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;DECLARE
	@TableName SYSNAME = &apos;MyTable&apos;,
	@TableSchema SYSNAME = &apos;dbo&apos;

IF OBJECT_ID(&apos;tempdb..#Destination&apos;) IS NOT NULL
	DROP TABLE #Destination

SELECT
	[ColumnId] = sc.[column_id],
	[ColumnName] = sc.[name] COLLATE DATABASE_DEFAULT,
	-- adjust timestamp type to varbinary
	[Type] = CASE WHEN st.[user_type_id] = 189 THEN &apos;varbinary&apos; ELSE st.[name] END COLLATE DATABASE_DEFAULT,
	-- adjust nvarchar length to half what is reported since it takes 2 bytes to store unicode, and adjust timestamp to the translated varbinary length
	[Length] = CASE WHEN sc.[max_length] = -1 THEN -1 WHEN st.[user_type_id] = 231 THEN sc.[max_length] / 2 WHEN st.[user_type_id] = 189 THEN 85 ELSE sc.[max_length] END,
	[Precision] = sc.[precision], 
	[Scale] = sc.[scale],
	[IsNullable] = sc.[is_nullable]
INTO #Destination
FROM [sys].[objects] so
JOIN [sys].[columns] sc
ON so.[object_id] = sc.[object_id]
JOIN [sys].[types] st
ON sc.[user_type_id] = st.[user_type_id]
JOIN [sys].[schemas] ss
ON so.[schema_id] = ss.[schema_id]
WHERE so.[name] = @TableName AND ss.[name] = @TableSchema
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Example -- Create Table Copy&lt;/h1&gt;
&lt;p&gt;Create a copy of the table using the source schema information possibly injecting extra columns in the copy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DECLARE
	@TableName SYSNAME = &apos;MyTable&apos;,
	@TableSchema SYSNAME = &apos;dbo&apos;,
	@IsDebug BIT = 0

DECLARE
	@DataType NVARCHAR(200),
	@Length INT,
	@Precision INT,
	@Scale INT,
	@IsNullable INT,
	@ColumnName NVARCHAR(200),
	@Statement NVARCHAR(MAX) = &apos;&apos;

DECLARE x CURSOR FAST_FORWARD FOR
	SELECT [ColumnName], [Type], [Length], [Precision], [Scale], [IsNullable]
	FROM #Source
	ORDER BY [ColumnId]

OPEN x
FETCH NEXT FROM x INTO @ColumnName, @DataType, @Length, @Precision, @Scale, @IsNullable
WHILE @@FETCH_STATUS = 0
BEGIN
	DECLARE @NullClause NVARCHAR(10) = &apos; NULL&apos;
	IF @IsNullable = 0
	BEGIN
		SELECT @NullClause = &apos; NOT NULL&apos;
	END

	IF @DataType = &apos;timestamp&apos;
	BEGIN
		SELECT @DataType = &apos;varbinary&apos;
		SELECT @Length = 85
	END

	IF @DataType IN(&apos;char&apos;,&apos;varchar&apos;,&apos;binary&apos;,&apos;varbinary&apos;,&apos;nvarchar&apos;,&apos;nchar&apos;)
	BEGIN
		SELECT @Statement = @Statement + &apos;[&apos; + @ColumnName + &apos;] &apos; + @DataType + &apos;(&apos; + ISNULL(NULLIF(CONVERT(NVARCHAR,@Length), &apos;-1&apos;), &apos;MAX&apos;) + &apos;)&apos; + @NullClause + &apos;,&apos;
	END
	ELSE IF @DataType IN(&apos;decimal&apos;,&apos;numeric&apos;)
	BEGIN
		SELECT @Statement = @Statement + &apos;[&apos; + @ColumnName + &apos;] &apos; + @DataType + &apos;(&apos; + CONVERT(NVARCHAR,@Precision) + &apos;,&apos; + CONVERT(NVARCHAR,@Scale) + &apos;)&apos;  + @NullClause + &apos;,&apos;
	END
	ELSE
	BEGIN
		SELECT @Statement = @Statement + &apos;[&apos; + @ColumnName + &apos;] &apos; + @DataType + @NullClause + &apos;,&apos;
	END
FETCH NEXT FROM x INTO @ColumnName, @DataType, @Length, @Precision, @Scale, @IsNullable
END
CLOSE x
DEALLOCATE x

SELECT @Statement = SUBSTRING(@Statement, 1, LEN(@Statement) - 1)

DECLARE @Sql NVARCHAR(MAX) = &apos;&apos;

SELECT @Sql = &apos;
CREATE TABLE [{0}].[{1}](
{2}
)&apos;

SELECT @Sql = REPLACE(@Sql, &apos;{0}&apos;, @TableSchema)
SELECT @Sql = REPLACE(@Sql, &apos;{1}&apos;, @TableName)
SELECT @Sql = REPLACE(@Sql, &apos;{2}&apos;, @Statement)

IF @IsDebug = 1
	PRINT (@Sql)
IF @IsDebug = 0
	EXEC (@Sql)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Example -- Schema Diff&lt;/h1&gt;
&lt;p&gt;Below are a few ways you can use this information to find;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;columns that have changed&lt;/li&gt;
&lt;li&gt;columns that are missing from the destination&lt;/li&gt;
&lt;li&gt;columns that have been removed from the source and not the destination&lt;/li&gt;
&lt;li&gt;columns that both tables have in common&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;IF OBJECT_ID(&apos;tempdb..#CHANGED_COLUMNS&apos;) IS NOT NULL
	DROP TABLE #CHANGED_COLUMNS

SELECT
    [ColumnName],
    [Type],
    [Length],
    [Precision], 
    [Scale]
INTO #CHANGED_COLUMNS
FROM #Source
EXCEPT
SELECT
    [ColumnName],
    [Type],
    [Length],
    [Precision], 
    [Scale]
FROM #Destination


IF OBJECT_ID(&apos;tempdb..#MISSING_COLUMNS&apos;) IS NOT NULL
	DROP TABLE #MISSING_COLUMNS

SELECT
    [ColumnName]
INTO #MISSING_COLUMNS
FROM #Source
EXCEPT
SELECT
    [ColumnName]
FROM #Destination

IF OBJECT_ID(&apos;tempdb..#DELETED_COLUMNS&apos;) IS NOT NULL
	DROP TABLE #DELETED_COLUMNS

SELECT
    [ColumnName]
INTO #DELETED_COLUMNS
FROM #Destination
EXCEPT
SELECT
    [ColumnName]
FROM #SOURCE

IF OBJECT_ID(&apos;tempdb..#SHARED_COLUMNS&apos;) IS NOT NULL
	DROP TABLE #SHARED_COLUMNS

SELECT
    [ColumnName]
INTO #SHARED_COLUMNS
FROM #Source
INTERSECT
SELECT
    [ColumnName]
FROM #Destination
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Example -- Sync Destination Schema with Source&lt;/h1&gt;
&lt;p&gt;Here we will cursor over the columns that are missing and either create a series of &lt;code&gt;ALTER TABLE&lt;/code&gt; statements to bring the destination table in sync with the source.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DECLARE
	@TableName SYSNAME = &apos;MyTable&apos;,
	@TableSchema SYSNAME = &apos;dbo&apos;,
	@IsDebug BIT = 0

IF OBJECT_ID(&apos;tempdb..#CHANGED_COLUMNS&apos;) IS NOT NULL
	DROP TABLE #CHANGED_COLUMNS

SELECT
    [ColumnName],
    [Type],
    [Length],
    [Precision], 
    [Scale]
INTO #CHANGED_COLUMNS
FROM #Source
EXCEPT
SELECT
    [ColumnName],
    [Type],
    [Length],
    [Precision], 
    [Scale]
FROM #Destination

DECLARE
	@Statement NVARCHAR(MAX) = &apos;&apos;,
	@AlterColumnStatement NVARCHAR(MAX) = &apos;&apos;,
	@ColumnName NVARCHAR(200),
	@Sql NVARCHAR(MAX) = &apos;&apos;

IF EXISTS (SELECT 1 FROM #CHANGED_COLUMNS)
BEGIN
	DECLARE 
		@DataType NVARCHAR(200),
		@Length INT,
		@Precision INT,
		@Scale INT

	DECLARE
		@HasExistingColumns INT = 0,
		@HasNewColumns INT = 0

	DECLARE x CURSOR FOR
		SELECT [ColumnName], [Type], [Length], [Precision], [Scale]
		FROM #COLUMNS
		ORDER BY 1

	OPEN x
	FETCH NEXT FROM x INTO @ColumnName, @DataType, @Length, @Precision, @Scale
	WHILE @@FETCH_STATUS = 0
	BEGIN
		DECLARE @Part NVARCHAR(MAX)
		DECLARE @AlterColumnStub NVARCHAR(MAX) = &apos;
ALTER TABLE [{2}].[{0}]
ALTER COLUMN {1}&apos;

		DECLARE @ColumnExists INT = 0
		IF EXISTS(SELECT 1 FROM #Destination WHERE [ColumnName] = @ColumnName)
		BEGIN
			SELECT @ColumnExists = 1
			SELECT @HasExistingColumns = 1
		END
		ELSE
		BEGIN
			SELECT @HasNewColumns = 1
		END

		IF @DataType = &apos;timestamp&apos;
		BEGIN
			SELECT @DataType = &apos;varbinary&apos;
			SELECT @Length = 85
		END

		IF @DataType IN(&apos;char&apos;,&apos;varchar&apos;,&apos;binary&apos;,&apos;varbinary&apos;,&apos;nvarchar&apos;,&apos;nchar&apos;)
		BEGIN
			SELECT @Part = &apos;[&apos; + @ColumnName + &apos;] &apos; + @DataType + &apos;(&apos; + ISNULL(NULLIF(CONVERT(NVARCHAR,@Length), &apos;-1&apos;), &apos;MAX&apos;) + &apos;)&apos;

			IF(@ColumnExists = 1)
			BEGIN
				SELECT @AlterColumnStatement = @AlterColumnStatement + REPLACE(@AlterColumnStub, &apos;{1}&apos;, @Part)
			END
			ELSE
			BEGIN
				SELECT @Statement = @Statement + @Part + &apos;,&apos;
			END
		END
		ELSE IF @DataType IN(&apos;decimal&apos;,&apos;numeric&apos;)
		BEGIN
			SELECT @Part = &apos;[&apos; + @ColumnName + &apos;] &apos; + @DataType + &apos;(&apos; + CONVERT(NVARCHAR,@Precision) + &apos;,&apos; + CONVERT(NVARCHAR,@Scale) + &apos;)&apos;

			IF(@ColumnExists = 1)
			BEGIN
				SELECT @AlterColumnStatement = @AlterColumnStatement + REPLACE(@AlterColumnStub, &apos;{1}&apos;, @Part)
			END
			ELSE
			BEGIN
				SELECT @Statement = @Statement + @Part + &apos;,&apos;
			END
		END
		ELSE
		BEGIN
			SELECT @Part = &apos;[&apos; + @ColumnName + &apos;] &apos; + @DataType

			IF(@ColumnExists = 1)
			BEGIN
				SELECT @AlterColumnStatement = @AlterColumnStatement + REPLACE(@AlterColumnStub, &apos;{1}&apos;, @Part)
			END
			ELSE
			BEGIN
				SELECT @Statement = @Statement + @Part + &apos;,&apos;
			END
		END
	FETCH NEXT FROM x INTO @ColumnName, @DataType, @Length, @Precision, @Scale
	END
	CLOSE x
	DEALLOCATE x

	IF (LEN(@Statement) &amp;gt; 1)
	BEGIN
		SELECT @Statement = SUBSTRING(@Statement, 1, LEN(@Statement) - 1)
	END

	-- determine if is alter or create statement
	IF OBJECT_ID(@EntityName) IS NOT NULL
	BEGIN
		IF (@HasExistingColumns = 1)
		BEGIN
			SELECT @Sql += @AlterColumnStatement
		END
		
		IF (@HasNewColumns = 1)
		BEGIN
			SELECT @Sql += &apos;
ALTER TABLE [{2}].[{0}]
ADD {1}&apos;
		END
	END

	SELECT @Sql = REPLACE(@Sql, &apos;{0}&apos;, @TableName)
	SELECT @Sql = REPLACE(@Sql, &apos;{1}&apos;, @Statement)
	SELECT @Sql = REPLACE(@Sql, &apos;{2}&apos;, @TableSchema)

    IF @IsDebug = 1
        PRINT (@Sql)
    IF @IsDebug = 0
        EXEC (@Sql)
END
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Example -- Generate Statement Layouts&lt;/h1&gt;
&lt;p&gt;Here we generate layouts that can be used in &lt;code&gt;SELECT&lt;/code&gt;, &lt;code&gt;INSERT&lt;/code&gt; and &lt;code&gt;UPDATE&lt;/code&gt; statements assuming the source table is aliased as &lt;code&gt;[source]&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IF OBJECT_ID(&apos;tempdb..#SHARED_COLUMNS&apos;) IS NOT NULL
	DROP TABLE #SHARED_COLUMNS

SELECT
    [ColumnName]
INTO #SHARED_COLUMNS
FROM #Source
INTERSECT
SELECT
    [ColumnName]
FROM #Destination

-- generate layout --
DECLARE
    @ColumnFormat NVARCHAR(MAX) = &apos;&apos;,
    @UpdateFormat NVARCHAR(MAX) = &apos;&apos;,
    @ValuesFormat NVARCHAR(MAX) = &apos;&apos;

SELECT
    @ColumnFormat = STUFF((SELECT &apos;, [&apos; + [ColumnName] + &apos;]&apos; FROM #SHARED_COLUMNS ORDER BY [ColumnName] FOR XML PATH(&apos;&apos;)), 1, 2, &apos;&apos;),
    @UpdateFormat = STUFF((SELECT &apos;, [&apos; + [ColumnName] + &apos;] = [source].[&apos; + [ColumnName] + &apos;]&apos; FROM #SHARED_COLUMNS ORDER BY [ColumnName] FOR XML PATH(&apos;&apos;)), 1, 2, &apos;&apos;),
    @ValuesFormat = STUFF((SELECT &apos;, [source].[&apos; + [ColumnName] + &apos;]&apos; FROM #SHARED_COLUMNS ORDER BY [ColumnName] FOR XML PATH(&apos;&apos;)), 1, 2, &apos;&apos;)

PRINT @ColumnFormat
PRINT @UpdateFormat
PRINT @ValuesFormat
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item><item><title>SQL Server Temporal Tables</title><link>https://www.kichka.dev/posts/sql-temporal-tables/</link><guid isPermaLink="true">https://www.kichka.dev/posts/sql-temporal-tables/</guid><description>SQL snips for querying and enabling temporal tables</description><pubDate>Tue, 23 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;System Version Query -- How To&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;SELECT * 
FROM dbo.MyTable

--( at time zone does not work on literals, use var to work around this limitation )--
DECLARE @AsOf DATETIME2 = &apos;4/8/2019 9:43AM&apos;
SELECT @AsOf = @AsOf AT TIME ZONE &apos;Eastern Standard Time&apos; AT TIME ZONE &apos;UTC&apos;

SELECT *
FROM dbo.MyTable
FOR SYSTEM_TIME AS OF @AsOf --( includes changes where the record was valid during the given time )--
--FOR SYSTEM_TIME ALL --( includes all changes )--
--FOR SYSTEM_TIME FROM @StartDate TO @EndDate --( exclusive of start and end dates )--
--FOR SYSTEM_TIME BETWEEN @StartDate AND @EndDate --( inclusive of start and end dates )--

--( query directly from the history table )--
SELECT *
FROM dbo.MyTableHistory
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Enable System Versioning&lt;/h1&gt;
&lt;p&gt;This snippet will add the required columns &lt;code&gt;ValidFrom&lt;/code&gt; and &lt;code&gt;ValidTo&lt;/code&gt; and enable system versioning with a history table of &lt;code&gt;{Schema}.{TableName}History&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DECLARE
	@TableName SYSNAME = &apos;MyTable&apos;,
	@TableSchema SYSNAME = &apos;dbo&apos;,
	@IsDebug BIT = 0

DECLARE @Sql NVARCHAR(MAX) = &apos;&apos;

--( add sys version columns and enable )--
SELECT @Sql = &apos;
ALTER TABLE [{0}].[{1}]
ADD [ValidFrom] DATETIME2 GENERATED ALWAYS AS ROW START HIDDEN NOT NULL CONSTRAINT [DF[{0}]].[{1}]].[ValidFrom]]]  DEFAULT (sysutcdatetime()),
[ValidTo] DATETIME2 GENERATED ALWAYS AS ROW END HIDDEN NOT NULL CONSTRAINT [DF[{0}]].[{1}]].[ValidTo]]] DEFAULT (CONVERT([datetime2](0),&apos;&apos;9999-12-31 23:59:59&apos;&apos;)),
PERIOD FOR SYSTEM_TIME([ValidFrom], [ValidTo])

ALTER TABLE [{0}].[{1}]
SET (SYSTEM_VERSIONING = ON (HISTORY_TABLE = [{0}].[{1}History]));
&apos;
SELECT @Sql = REPLACE(@Sql, &apos;{0}&apos;, @TableSchema)
SELECT @Sql = REPLACE(@Sql, &apos;{1}&apos;, @TableName)

IF @IsDebug = 1
	PRINT (@Sql)
IF @IsDebug = 0
	EXEC (@Sql)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Disable System Versioning&lt;/h1&gt;
&lt;p&gt;This will disable system versioning on the given table.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DECLARE
	@TableName SYSNAME = &apos;MyTable&apos;,
	@TableSchema SYSNAME = &apos;dbo&apos;,
	@IsDebug BIT = 0

DECLARE @Sql NVARCHAR(MAX) = &apos;&apos;

SELECT @Sql = &apos;
ALTER TABLE [{0}].[{1}]
SET (SYSTEM_VERSIONING = OFF)
&apos;
SELECT @Sql = REPLACE(@Sql, &apos;{0}&apos;, @TableSchema)
SELECT @Sql = REPLACE(@Sql, &apos;{1}&apos;, @TableName)

IF @IsDebug = 1
	PRINT (@Sql)
IF @IsDebug = 0
	EXEC (@Sql)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item><item><title>SQL Server See What&apos;s Running</title><link>https://www.kichka.dev/posts/sql-see-whats-running/</link><guid isPermaLink="true">https://www.kichka.dev/posts/sql-see-whats-running/</guid><description>This is a collection of SQL snippets for tracking down what is running and what might be misbehaving</description><pubDate>Mon, 22 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Below are a collection of SQL query that I use to diagnose and trace what is running on the server.&lt;/p&gt;
&lt;p&gt;This is my goto snippet to quickly identify what is running and the current state of the server. It will provide you cpu and disk read time as well as the query plans which can be used to identify problem children.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DROP TABLE #PERFSNAP

SELECT
    r.cpu_time,
    r.logical_reads,
    r.session_id 
INTO #TEMP
FROM sys.dm_exec_sessions s 
JOIN sys.dm_exec_requests r 
ON s.session_id = r.session_id
--AND s.last_request_start_time = r.start_time
WHERE is_user_process = 1 
 
WAITFOR DELAY &apos;00:00:01&apos;

SELECT 
    [text] = SUBSTRING(
        h.text, 
        (r.statement_start_offset / 2) + 1,
        ((  CASE r.statement_end_offset 
            WHEN -1 
            THEN DATALENGTH(h.text)
            ELSE r.statement_end_offset
            END - r.statement_start_offset) / 2) + 1
    ),
    [cpu_diff] = r.cpu_time - t.cpu_time, 
    [read_diff] = r.logical_reads - t.logical_reads,
    p.query_plan,
    r.wait_type,
    r.wait_time,
    r.last_wait_type,
    r.wait_resource,
    r.command,
    [database] = DB_NAME(r.database_id),
    r.blocking_session_id,
    r.granted_query_memory,
    r.session_id,
    r.reads,
    r.writes,
    r.row_count,
    s.[host_name],
    s.program_name,
    s.login_name
INTO #PERFSNAP
FROM sys.dm_exec_sessions s 
JOIN sys.dm_exec_requests r 
ON s.session_id = r.session_id
AND s.last_request_start_time = r.start_time
FULL JOIN #TEMP t 
ON t.session_id = s.session_id
CROSS APPLY sys.dm_exec_sql_text(r.sql_handle) h
CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) p
ORDER BY 3 desc

DROP TABLE #TEMP

SELECT * FROM #PERFSNAP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is another variation of the above code snippet just without the cpu and read timing, and query plans.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;BEGIN
    -- Do not lock anything, and do not get held up by any locks.
    SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED

    -- What SQL Statements Are Currently Running?
    SELECT
        [spid] = session_Id,
        ecid,
        [database] = DB_NAME(sp.dbid),
        [user] = nt_username,
        [status] = er.status,
        [wait] = wait_type,
        [individual_query] = SUBSTRING(
            qt.text, 
            er.statement_start_offset / 2,
            (   CASE 
                WHEN er.statement_end_offset = -1
                THEN LEN(CONVERT(NVARCHAR(MAX), qt.text)) * 2
                ELSE er.statement_end_offset 
                END - er.statement_start_offset
            ) / 2
        ),
        [parent_query] = qt.text,
        program_name,
        hostname,
        nt_domain,
        start_time
    FROM sys.dm_exec_requests er
    JOIN sys.sysprocesses sp
    ON er.session_id = sp.spid
    CROSS APPLY sys.dm_exec_sql_text(er.sql_handle) qt
    WHERE session_Id &amp;gt; 50 -- ignore system spids
    AND session_Id NOT IN (@@SPID) -- ignore this current statement
    --AND session_Id IN ( 126 )
    --AND DB_NAME(sp.dbid) = &apos;&apos;
    ORDER BY 1, 2
END

--EXECUTE sp_who2 85
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These snips will help track down different types of wait resources and their reasons.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--KEY: 8:281474980315136 (4102a73ebc59)
--( Key Wait Resource )--
SELECT 
    [TableName] = o.name, 
    [IndexName] = i.name,
    [SchemaName] = SCHEMA_NAME(o.schema_id)
FROM sys.partitions p 
JOIN sys.objects o 
ON p.OBJECT_ID = o.OBJECT_ID 
JOIN sys.indexes i 
ON p.OBJECT_ID = i.OBJECT_ID
AND p.index_id = i.index_id 
WHERE p.hobt_id = 281474980315136

--( Page Wait Resource )--
SELECT DB_NAME(68)

-- select the database based on the output above statement then execute the below statement
DBCC traceon (3604)
GO
-- Database_id, file_id, page_id 
DBCC page (68, 1, 492478) 

--( Object Wait Resource )--
--OBJECT: 8:1954106002:0 
SELECT OBJECT_NAME(1954106002)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item><item><title>SQL Server Shell Games</title><link>https://www.kichka.dev/posts/sql-shell-games/</link><guid isPermaLink="true">https://www.kichka.dev/posts/sql-shell-games/</guid><description>DELETE large amounts of data from a huge database table with this one simple trick</description><pubDate>Sat, 20 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;SQL Partition SWITCH Operation&lt;/h1&gt;
&lt;p&gt;With the power of the table partitioning &lt;code&gt;SWITCH&lt;/code&gt; operation you can swap table metadata pointers instantly.&lt;/p&gt;
&lt;p&gt;This can be used on any modern edition of sql server, even if table partioning is not supported. This is because under the hood every table is a single partition. With this you can quickly move data to an empty table and with some &quot;shell games&quot; instantly swap back to the old table.&lt;/p&gt;
&lt;p&gt;I have used this many times to remove billions of rows by selecting out only the few hundred thousand that I wanted to keep into a new table and swapping table partition pointers. The swap is instant and there is little to no downtime.&lt;/p&gt;
&lt;p&gt;Let&apos;s walk through an example of the process.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-- create test set of tables
CREATE TABLE dbo.TEST1(id INT)
CREATE TABLE dbo.TEST2(id INT)

-- populate test1 with 10000 rows of data
DECLARE @i INT = 0
WHILE @i &amp;lt; 10000
BEGIN
    INSERT INTO dbo.TEST1(id)
    VALUES(@i)
    SELECT @i += 1
END

-- view current state of things
SELECT * FROM dbo.TEST1
SELECT * FROM dbo.TEST2

-- switch data to second table
ALTER TABLE dbo.TEST1
SWITCH TO dbo.TEST2

-- view current state of things
SELECT * FROM dbo.TEST1
SELECT * FROM dbo.TEST2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have an idea of how this works, you can imagine that you have a table with billions of rows that you want to delete. DELETEs have to acquire a lock on the table at some level potentially preventing others from interacting with it. SELECTs however are much quicker and can be done in parallel without affecting others.&lt;/p&gt;
&lt;p&gt;Below, we select out just the handful of rows that we want, truncate the original table, then swap the table pointers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-- =========================
-- TRUNCATE TABLE SHELL GAME
-- =========================
-- move out set of data i want to keep
SELECT TOP 1000 * INTO dbo.TEMP_SHELL FROM dbo.TEST2

-- truncate orig table
TRUNCATE TABLE dbo.TEST2

-- move data metadata from shell to orig table
ALTER TABLE dbo.TEMP_SHELL
SWITCH TO dbo.TEST2

-- view current state of things
SELECT * FROM dbo.TEMP_SHELL
SELECT * FROM dbo.TEST2

-- remove shell table
DROP TABLE dbo.TEMP_SHELL

-- view current state of things
SELECT * FROM dbo.TEST1
SELECT * FROM dbo.TEST2

-- cleanup
DROP TABLE dbo.TEST1
DROP TABLE dbo.TEST2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This process has saved me countless hours of data processing and downtime.&lt;/p&gt;
&lt;p&gt;I have used it in ETLs, ad-hoc table cleanups, as well as to create a simple data archiving process that moves old data by quarter into another table for historical reporting keeping the primary table lean.&lt;/p&gt;
&lt;p&gt;This may not be such a concern now that Microsoft has opened up the table partitioning feature to all editions of SQL Server, but still useful for situations where the complexity of managing table partitioning outweighs the benifits.&lt;/p&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item><item><title>Start of a Journey</title><link>https://www.kichka.dev/posts/start-of-a-journey/</link><guid isPermaLink="true">https://www.kichka.dev/posts/start-of-a-journey/</guid><description>Today I embark on a new journey of writing down my thoughts and experiences in software development</description><pubDate>Mon, 15 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;A new Beginning&lt;/h1&gt;
&lt;p&gt;For a long time now I have been wanting to start writing my thoughts down on paper. I have a physical notebook where my thoughts and notes are haphazardly written which has this big downside of not being very searchable.&lt;/p&gt;
&lt;p&gt;So over the next couple of weeks I will be working on digitizing these notes and thoughts here.&lt;/p&gt;
&lt;p&gt;Come join me on this journey of inspiration lost and found.&lt;/p&gt;
&lt;p&gt;~ SK&lt;/p&gt;
</content:encoded></item></channel></rss>